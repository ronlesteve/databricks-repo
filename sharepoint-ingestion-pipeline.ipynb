{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4064d4c1-e99d-4498-8f58-1c1c14598126",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "\n",
    "# This sets up the API utils for creating managed ingestion pipelines in Databricks.\n",
    "\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "notebook_context = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "api_token = notebook_context.apiToken().get()\n",
    "workspace_url = notebook_context.apiUrl().get()\n",
    "api_url = f\"{workspace_url}/api/2.0/pipelines\"\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Bearer {}'.format(api_token),\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "\n",
    "def check_response(response):\n",
    "    if response.status_code == 200:\n",
    "        print(\"Response from API:\\n{}\".format(json.dumps(response.json(), indent=2, sort_keys=False)))\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data: error_code={response.status_code}, error_message={response.json().get('message', response.text)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_pipeline(pipeline_definition: str):\n",
    "  response = requests.post(url=api_url, headers=headers, data=pipeline_definition)\n",
    "  check_response(response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def edit_pipeline(id: str, pipeline_definition: str): \n",
    "  response = requests.put(url=f\"{api_url}/{id}\", headers=headers, data=pipeline_definition)\n",
    "  check_response(response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def delete_pipeline(id: str): \n",
    "  response = requests.delete(url=f\"{api_url}/{id}\", headers=headers)\n",
    "  check_response(response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def list_pipeline(filter: str):\n",
    "  body = \"\" if len(filter) == 0 else f\"\"\"{{\"filter\": \"{filter}\"}}\"\"\"\n",
    "  response = requests.get(url=api_url, headers=headers, data=body)\n",
    "  check_response(response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_pipeline(id: str):\n",
    "  response = requests.get(url=f\"{api_url}/{id}\", headers=headers)\n",
    "  check_response(response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def start_pipeline(id: str, full_refresh: bool=False):\n",
    "  body = f\"\"\"\n",
    "  {{\n",
    "    \"full_refresh\": {str(full_refresh).lower()},\n",
    "    \"validate_only\": false,\n",
    "    \"cause\": \"API_CALL\"\n",
    "  }}\n",
    "  \"\"\"\n",
    "  response = requests.post(url=f\"{api_url}/{id}/updates\", headers=headers, data=body)\n",
    "  check_response(response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stop_pipeline(id: str):\n",
    "  print(\"cannot stop pipeline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b92f1cf-592d-46ca-b000-0bf987c9fa4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Do not modify the PREVIEW channel in the code below. \n",
    "\n",
    "\n",
    "# If you want to ingest all drives in your SharePoint site, use the schema spec. If you want to ingest only some drives in your SharePoint site, use the table spec.\n",
    "\n",
    "\n",
    "# By default, the API will use SCD type 1 for the data. This means that it will overwrite the file in the destination if itâ€™s edited in the source. If you prefer to preserve historical file versions and use SCD type 2, then you should specify that in the config.\n",
    "\n",
    "\n",
    "# schema spec:\n",
    "#source_schema = SHAREPOINT SITE ID (Enter this URL in browser to find SITE ID https://{yourcompanyname}.sharepoint.com/sites/{yourcompanysite}/_api/site/id). The Edm.Guid will be the SITE ID\n",
    "#source_table = SHAREPOINT DRIVE NAME\n",
    "\n",
    "pipeline_spec = \"\"\"\n",
    "{\n",
    " \"name\": \"sharepoint_ingestion_pipeline\",\n",
    " \"ingestion_definition\": {\n",
    "     \"connection_name\": \"sharepoint_oauth\",\n",
    "     \"objects\": [\n",
    "        {\n",
    "          \"schema\": {\n",
    "            \"source_schema\": \"ENTER-YOUR-SHAREPOINT-SITE-ID\", \n",
    "            \"source_table\": \"Shared Documents\",\n",
    "            \"destination_catalog\": \"sandbox\",\n",
    "            \"destination_schema\": \"bronze\",\n",
    "            \"destination_table\": \"documents\",\n",
    "            \"table_configuration\": {\n",
    "              \"scd_type\": \"SCD_TYPE_1\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    " },\n",
    " \"channel\": \"PREVIEW\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "create_pipeline(pipeline_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca93632-9a71-4c30-840f-d66168ba4bb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_pipeline(\"ENTER-YOUR-PIPELINE-ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "379650f9-43b2-4fc3-999c-ccc13af530bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the desired file content\n",
    "from pyspark.sql.functions import col, length, isnull\n",
    "\n",
    "# Read from bronze table and filter for your Excel file\n",
    "df = spark.read.table(\"sandbox.bronze.documents\")\n",
    "\n",
    "excel_file_check = df.filter(col('file_metadata.name').contains(\"ENTER-YOUR-SHAREPOINT-EXCEL-FILE.xlsx\")).select(\n",
    "    col(\"file_id\"),\n",
    "    col(\"file_metadata.name\").alias(\"file_name\"),\n",
    "    col(\"file_metadata.size_in_bytes\").alias(\"file_size\"),\n",
    "    col(\"source_metadata.mime_type\").alias(\"mime_type\"),\n",
    "    isnull(col(\"content.inline_content\")).alias(\"inline_content_is_null\"),\n",
    "    length(col(\"content.inline_content\")).alias(\"inline_content_length\"),\n",
    "    col(\"content.content_file_path\").alias(\"content_file_path\"),\n",
    "    isnull(col(\"content.content_file_path\")).alias(\"file_path_is_null\")\n",
    ")\n",
    "\n",
    "display(excel_file_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b958489-0d27-4817-9a99-306237cc24c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl\n",
    "# Auto type-preserving Excel import with safe null handling\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from pyspark.sql.functions import col, to_date, coalesce\n",
    "\n",
    "# Step 1: Read from bronze table\n",
    "df = spark.read.table(\"sandbox.bronze.documents\")\n",
    "\n",
    "excel_row = (\n",
    "    df.filter(col('file_metadata.name').contains(\"ENTER-YOUR-SHAREPOINT-EXCEL-FILE.xlsx\"))\n",
    "      .select(col(\"content.inline_content\").alias(\"binary_content\"))\n",
    "      .first()\n",
    ")\n",
    "\n",
    "binary_content = excel_row[\"binary_content\"]\n",
    "\n",
    "# Step 2: Read Excel with pandas (auto type inference)\n",
    "excel_df = pd.read_excel(\n",
    "    BytesIO(binary_content),\n",
    "    sheet_name='SampleData',\n",
    "    engine='openpyxl',\n",
    ")\n",
    "\n",
    "print(f\"Read Excel: {excel_df.shape[0]} rows {excel_df.shape[1]} columns\")\n",
    "\n",
    "# Step 3: Replace NaN/NaT with None (preserves dtype)\n",
    "excel_df = excel_df.where(pd.notnull(excel_df), None)\n",
    "\n",
    "# Step 4: Convert to Spark DataFrame with inferred schema\n",
    "spark_excel_df = spark.createDataFrame(excel_df)\n",
    "print(\"Converted to Spark DataFrame (auto-inferred schema)\")\n",
    "spark_excel_df.printSchema()\n",
    "\n",
    "# Step 5: Dynamic typecasting for date columns\n",
    "date_patterns = [\"yyyy-MM-dd\", \"dd-MM-yyyy\", \"MM/dd/yyyy\"]\n",
    "def dynamic_date_cast(col_name):\n",
    "    return coalesce(\n",
    "        *[to_date(col(col_name), fmt) for fmt in date_patterns],\n",
    "        to_date(col(col_name))\n",
    "    ).alias(col_name)\n",
    "\n",
    "date_cols = [f.name for f in spark_excel_df.schema.fields if \"date\" in f.name.lower() or \"last_updated\" in f.name.lower()]\n",
    "other_cols = [col(c) for c in spark_excel_df.columns if c not in date_cols]\n",
    "\n",
    "final_df = spark_excel_df.select(\n",
    "    *other_cols,\n",
    "    *[dynamic_date_cast(c) for c in date_cols]\n",
    ")\n",
    "\n",
    "# Step 6: Write final table\n",
    "final_df.write.mode(\"overwrite\").saveAsTable(\"sandbox.silver.your_sharepoint_table_name\")\n",
    "\n",
    "# Step 7: Display the final table\n",
    "display(spark.read.table(\"sandbox.silver.your_sharepoint_table_name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82090c97-8f45-49c1-a99a-c80ddb178512",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delete_pipeline('ENTER-YOUR-PIPELINE-ID')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8534512227953462,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "sharepoint-ingestion-pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
