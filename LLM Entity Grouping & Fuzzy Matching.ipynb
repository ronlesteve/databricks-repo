{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05c9370e-a353-477f-980a-6a8edc7711c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create generic 3PL Accessorial Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f787e6a1-dca8-4094-a6cb-c3f7c84c5cd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "accessorial_data = [\n",
    "    (1,  \"Liftgate Service - Handling - CarrierA - Midwest - NoExp - NoTemp\"),\n",
    "    (2,  \"lift gate service - Handling - CarrierB - Midwest - NoExp - NoTemp\"),\n",
    "    (3,  \"Lift-Gate Service Charge - Handling - CarrierC - Midwest - NoExp - NoTemp\"),\n",
    "    (4,  \"Service Liftgate Handling - CarrierA - Midwest - NoExp - NoTemp\"),\n",
    "    (5,  \"Handling CarrierB Lift Gate Service - Midwest - NoExp - NoTemp\"),\n",
    "    (6,  \"LIFtgate - Handling - CarrierA - MidWest - NoExp - NoTemp\"),\n",
    "    (7,  \"Lifgtate Servic - Handling - CarrierB - Midwest - NoExp - NoTemp\"),\n",
    "    (8,  \"Residential Delivery - Delivery - CarrierA - Northeast - NoExp - NoTemp\"),\n",
    "    (9,  \"res. delivery charge - Delivery - CarrierC - Northeast - NoExp - NoTemp\"),\n",
    "    (10, \"Delivery Fee Residential - CarrierB - Northeast - NoExp - NoTemp\"),\n",
    "    (11, \"Residential Delivery Fee - Delivery - CarrierB - Northeast - NoExp - NoTemp\"),\n",
    "    (12, \"Expedited Shipping - Delivery - CarrierA - South - Exp - NoTemp\"),\n",
    "    (13, \"Ship Fee expedited - Delivery - CarrierB - South - Exp - NoTemp\"),\n",
    "    (14, \"Exp Ship Fee - Delivery - CarrierA - South - Exp - NoTemp\"),\n",
    "    (15, \"Pallet Jack Usage - Handling - Warehouse1 - Midwest - NoExp - NoTemp\"),\n",
    "    (16, \"Pallet Jack Use Fee - Handling - Warehouse1 - Midwest - NoExp - NoTemp\"),\n",
    "    (17, \"Usage Pallet Jack Fee - Warehouse1 - Midwest - NoExp - NoTemp\"),\n",
    "    (18, \"Temperature Controlled Fee - Handling - CarrierC - West - NoExp - Temp\"),\n",
    "    (19, \"Temp Ctrl Fee - Handling - CarrierC - West - NoExp - Temp\"),\n",
    "    (20, \"Temp Controlled Handling Fee - CarrierC - West - NoExp - Temp\"),\n",
    "    (21, \"Freight Bill Correction - Adjustment - BillingSys - All - NoExp - NoTemp\"),\n",
    "    (22, \"Freight Bill Corr. Fee - Adjustment - BillingSys - All - NoExp - NoTemp\"),\n",
    "    (23, \"Bill Correction Freight - Adjustment - BillingSys - All - NoExp - NoTemp\"),\n",
    "    (24, \"Lift Gate Delivery - Delivery - CarrierB - Midwest - NoExp - NoTemp\"),\n",
    "    (25, \"Delivery Lift Gate - CarrierB - Midwest - NoExp - NoTemp\"),\n",
    "    (26, \"Lift Gate Delvry Service - Delivery - CarrierB - Midwest - NoExp - NoTemp\"),\n",
    "    (27, \"Residential Delvry Fee - Delivery - CarrierA - Northeast - NoExp - NoTemp\"),\n",
    "    (28, \"Residential Delivery Extra Charge - Delivery - CarrierA - Northeast - NoExp - NoTemp\"),\n",
    "    (29, \"Express Shipping Service - Delivery - CarrierB - South - Exp - NoTemp\"),\n",
    "    (30, \"Express Ship Fee - Delivery - CarrierC - South - Exp - NoTemp\"),\n",
    "    (31, \"Temperature Control Charge - Handling - CarrierC - West - NoExp - Temp\"),\n",
    "    (32, \"Seasonal Salad - Appetizer - OutletA - Downtown - NoExp - NoTemp\"),\n",
    "    (33, \"Salad Mix with Nuts - Appetizer - OutletB - Uptown - NoExp - NoTemp\"),\n",
    "    (34, \"Fresh Green Salad with Dressings - Appetizer - OutletA - Downtown - NoExp - NoTemp\"),\n",
    "    (35, \"Classic Tomato Pie - Main - OutletC - Midtown - NoExp - NoTemp\"),\n",
    "    (36, \"Tomato Flatbread - Main - OutletC - Midtown - NoExp - NoTemp\"),\n",
    "    (37, \"Cheese Flatbread - Main - OutletD - Midtown - NoExp - NoTemp\"),\n",
    "    (38, \"Grilled Chicken Wrap - Main - OutletE - Downtown - NoExp - NoTemp\"),\n",
    "    (39, \"Chicken Wrap Grilled - Main - OutletE - Downtown - NoExp - NoTemp\"),\n",
    "    (40, \"Veggie Wrap - Main - OutletE - Downtown - NoExp - NoTemp\"),\n",
    "    (41, \"Vegan Wrap Deluxe - Main - OutletE - Downtown - NoExp - NoTemp\"),\n",
    "    (42, \"Chocolate Dessert Cake - Dessert - OutletF - Suburb - NoExp - NoTemp\"),\n",
    "    (43, \"Chocolate Fudge Cake - Dessert - OutletF - Suburb - NoExp - NoTemp\"),\n",
    "    (44, \"Caramel Ice Dessert - Dessert - OutletF - Suburb - NoExp - NoTemp\"),\n",
    "    (45, \"Vanilla Ice Dessert - Dessert - OutletF - Suburb - NoExp - NoTemp\"),\n",
    "    (46, \"Lemon Drink - Beverage - OutletA - Downtown - NoExp - NoTemp\"),\n",
    "    (47, \"Lemon Citrus Juice - Beverage - OutletA - Downtown - NoExp - NoTemp\"),\n",
    "    (48, \"Fresh Lemon Drink - Beverage - OutletA - Downtown - NoExp - NoTemp\"),\n",
    "    (49, \"Iced Herbal Tea - Beverage - OutletA - Downtown - NoExp - NoTemp\"),\n",
    "    (50, \"Herbal Tea Iced - Beverage - OutletA - Downtown - NoExp - NoTemp\"),\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"attribute\"]\n",
    "df_accessorial = spark.createDataFrame(accessorial_data, schema=columns)\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS sandbox.bronze\n",
    "    \"\"\"\n",
    ")\n",
    "df_accessorial.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sandbox.bronze.accessorials\")\n",
    "\n",
    "# Display the DataFrame interactively in the notebook\n",
    "display(df_accessorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05a18dc2-2427-411d-9ad6-20ade7e70742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Compose Prompt Instructions\n",
    "- Query AI Model\n",
    "- Parse & Save Response\n",
    "(with out MLFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70e49ad3-28d8-4d36-9b22-1fd632cf94eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Step 1: Load bronze accessorial data\n",
    "df = spark.table(\"sandbox.bronze.accessorials\").select(\"id\", \"attribute\")\n",
    "items_list = df.collect()\n",
    "\n",
    "# Step 2: Construct prompt input lines\n",
    "lines = [f\"{row.id}: {row.attribute}\" for row in items_list]\n",
    "data_str = \"\\n\".join(lines)\n",
    "\n",
    "# Step 3: Compose prompt with extended category logic for \"Snack\"\n",
    "prompt = f\"\"\"\n",
    "You are given a list of shipment accessorial service descriptions, each with a unique ID.\n",
    "\n",
    "Your task:\n",
    "1. Compute semantic similarity and fuzzy matches between descriptions.\n",
    "2. Group similar items into clusters representing the same entity despite typos or rearrangement.\n",
    "3. Assign each cluster a numeric entity_group_id starting from 1.\n",
    "4. Select a canonical representative description per cluster (shortest/clearest).\n",
    "5. For each item, if the description or grouped attribute relates to any kind of food or beverage, assign the category \"Snack\".\n",
    "   Otherwise, assign the category as the grouped_attribute value.\n",
    "6. Output a JSON array of objects with fields: id, attribute, entity_group_id, grouped_attribute, category.\n",
    "\n",
    "Provide ONLY the JSON array of objects.\n",
    "\n",
    "Data:\n",
    "{data_str}\n",
    "\"\"\"\n",
    "\n",
    "# Step 4: Register prompt as temp view\n",
    "spark.createDataFrame([(prompt,)], [\"prompt\"]).createOrReplaceTempView(\"prompt_table\")\n",
    "\n",
    "# Step 5: Query the AI model for output JSON\n",
    "result_df = spark.sql(\"\"\"\n",
    "SELECT ai_query(\n",
    "    'databricks-meta-llama-3-3-70b-instruct',\n",
    "    prompt\n",
    ") AS raw_json\n",
    "FROM prompt_table\n",
    "\"\"\")\n",
    "\n",
    "raw_json = result_df.collect()[0][\"raw_json\"]\n",
    "\n",
    "# Step 6: Parse JSON response safely and create Spark DataFrame\n",
    "if raw_json and raw_json.strip():\n",
    "    try:\n",
    "        raw_json_clean = raw_json.strip()\n",
    "        # Remove markdown code fences if present\n",
    "        if raw_json_clean.startswith(\"\"):\n",
    "            raw_json_clean = raw_json_clean.lstrip(\"`\").lstrip(\"json\").strip()\n",
    "            if raw_json_clean.endswith(\"\"):\n",
    "                raw_json_clean = raw_json_clean[:-3].strip()\n",
    "        parsed_list = json.loads(raw_json_clean)\n",
    "        \n",
    "        # Check if parse resulted in a list and has records\n",
    "        if isinstance(parsed_list, list) and len(parsed_list) > 0:\n",
    "            final_df = spark.createDataFrame(parsed_list)\n",
    "            \n",
    "            # Ensure the schema exists before saving the table\n",
    "            spark.sql(\"CREATE SCHEMA IF NOT EXISTS sandbox.gold\")\n",
    "\n",
    "            # Persist DataFrame to Gold Delta table with overwrite mode\n",
    "            final_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"sandbox.gold.accessorials\")\n",
    "            \n",
    "            # Query and display Gold table in tabular format\n",
    "            gold_df = spark.sql(\"SELECT * FROM sandbox.gold.accessorials\")\n",
    "            display(gold_df)\n",
    "        else:\n",
    "            print(\"Parsed JSON is not a valid list or is empty.\")\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error parsing JSON from LLM response:\")\n",
    "        print(e)\n",
    "        print(\"Raw JSON output was:\")\n",
    "        print(raw_json_clean)\n",
    "else:\n",
    "    print(\"LLM response empty or only whitespace - check prompt or model output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02de810a-c0c2-4983-8458-2730ca1cb891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Compose Prompt Instructions\n",
    "- Query AI Model\n",
    "- Parse & Save Response\n",
    "(with MLFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5dfc8fa-58ed-4178-8340-5eb80024a971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import mlflow\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def strip_markdown_fences(text):\n",
    "    # Remove markdown code fences and language tags if present\n",
    "    return re.sub(r\"^(\\s*json||json)\", \"\", text, flags=re.MULTILINE).strip()\n",
    "\n",
    "def extract_json_and_reasoning(text):\n",
    "    # Extract JSON substring and reasoning text (everything before JSON)\n",
    "    start = text.find('[')\n",
    "    end = text.rfind(']')\n",
    "    if start == -1 or end == -1:\n",
    "        return None, text\n",
    "    json_part = text[start:end+1]\n",
    "    reasoning_part = text[:start].strip()\n",
    "    return json_part, reasoning_part\n",
    "\n",
    "# Step 1: Load bronze accessorial data\n",
    "df = spark.table(\"sandbox.bronze.accessorials\").select(\"id\", \"attribute\")\n",
    "items_list = df.collect()\n",
    "\n",
    "# Step 2: Prepare prompt input lines\n",
    "lines = [f\"{row.id}: {row.attribute}\" for row in items_list]\n",
    "data_str = \"\\n\".join(lines)\n",
    "\n",
    "# Step 3: Compose prompt with explicit reasoning request\n",
    "prompt = f\"\"\"\n",
    "You are given a list of shipment accessorial service descriptions, each with a unique ID.\n",
    "\n",
    "Your task:\n",
    "1. Compute semantic similarity and fuzzy matches between descriptions.\n",
    "2. Group similar items into clusters representing the same entity despite typos or rearrangement.\n",
    "3. Assign each cluster a numeric entity_group_id starting from 1.\n",
    "4. Select a canonical representative description per cluster (shortest/clearest).\n",
    "5. For each item, if the description or grouped attribute relates to any kind of food or beverage, assign the category \"Snack\".\n",
    "   Otherwise, assign the category as the grouped_attribute value.\n",
    "6. Output a JSON array of objects with fields: id, attribute, entity_group_id, grouped_attribute, category.\n",
    "\n",
    "Additionally, provide a detailed explanation of your reasoning and the steps you took to group and categorize the items, including examples of how you handled typos, rearrangements, and category assignments. Place your reasoning before the JSON output.\n",
    "\n",
    "Provide your reasoning first, then the JSON array of objects.\n",
    "\n",
    "Data:\n",
    "{data_str}\n",
    "\"\"\"\n",
    "\n",
    "spark.createDataFrame([(prompt,)], [\"prompt\"]).createOrReplaceTempView(\"prompt_table\")\n",
    "\n",
    "# Step 4: Query the AI model for the output JSON and reasoning\n",
    "result_df = spark.sql(\"\"\"\n",
    "SELECT ai_query(\n",
    "    'databricks-meta-llama-3-3-70b-instruct',\n",
    "    prompt\n",
    ") AS raw_output\n",
    "FROM prompt_table\n",
    "\"\"\")\n",
    "\n",
    "raw_output = result_df.collect()[0][\"raw_output\"]\n",
    "\n",
    "# Step 5: extract reasoning and JSON parts\n",
    "clean_output = strip_markdown_fences(raw_output)\n",
    "json_str, reasoning = extract_json_and_reasoning(clean_output)\n",
    "\n",
    "# Step 6: Log reasoning and prompt in MLflow experiment but do not print them\n",
    "with mlflow.start_run(run_name=\"llm_accessorials_reasoning_log_only\") as run:\n",
    "    mlflow.log_text(f\"PROMPT:\\n{prompt}\\n\\nREASONING:\\n{reasoning or ''}\", \"llm_reasoning.txt\")\n",
    "\n",
    "# Step 7: Parse JSON and proceed like original flow (unchanged)\n",
    "if json_str and json_str.strip():\n",
    "    try:\n",
    "        parsed_list = json.loads(json_str)\n",
    "        if isinstance(parsed_list, list) and len(parsed_list) > 0:\n",
    "            final_df = spark.createDataFrame(parsed_list)\n",
    "\n",
    "            spark.sql(\"CREATE SCHEMA IF NOT EXISTS sandbox.gold\")\n",
    "            final_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"sandbox.gold.accessorials\")\n",
    "\n",
    "            # Display the Gold table interactively\n",
    "            gold_df = spark.sql(\"SELECT * FROM sandbox.gold.accessorials\")\n",
    "            display(gold_df)\n",
    "        else:\n",
    "            print(\"Parsed JSON is not a valid list or is empty.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error parsing JSON from LLM response:\")\n",
    "        print(e)\n",
    "        print(\"Raw JSON output was:\")\n",
    "        print(json_str)\n",
    "else:\n",
    "    print(\"LLM response empty or only whitespace - check prompt or model output.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LLM Entity Grouping & Fuzzy Matching",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
