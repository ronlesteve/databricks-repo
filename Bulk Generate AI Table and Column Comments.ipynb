{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c051cbf0-398c-4336-b0a4-f8630ac7b78a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bulk AI Generate all Table descriptions for a specified Catalog and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a6189ab-2b2f-4126-9a23-22125dbffa76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = 'sandbox'\n",
    "schemas = ['bronze', 'silver', 'gold']\n",
    "\n",
    "def sanitize_comment(text):\n",
    "    if not text:\n",
    "        return \"No description available.\"\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "    text = text.replace(\"'\", \"''\")\n",
    "    text = text.replace(\"`\", \"\")\n",
    "    return text\n",
    "\n",
    "def is_valid_identifier(name):\n",
    "    # Only allow alphanumeric and underscores, and must not start with a digit\n",
    "    import re\n",
    "    return re.match(r'^[A-Za-z_][A-Za-z0-9_]*$', name) is not None\n",
    "\n",
    "processed_tables = []\n",
    "\n",
    "print(f\"Found schemas: {schemas}\")\n",
    "\n",
    "for schema in schemas:\n",
    "    tables_df = spark.sql(f\"\"\"\n",
    "        SELECT DISTINCT table_name\n",
    "        FROM {catalog}.information_schema.tables\n",
    "        WHERE table_catalog = '{catalog}'\n",
    "          AND table_schema = '{schema}'\n",
    "          AND lower(table_name) NOT LIKE 'information_schema%'\n",
    "          AND lower(table_name) NOT LIKE 'sys%'\n",
    "          AND lower(table_name) NOT LIKE 'system%'\n",
    "    \"\"\")\n",
    "    tables = [row['table_name'] for row in tables_df.collect()]\n",
    "    print(f\"Schema {schema} has tables: {tables}\")\n",
    "\n",
    "    for table in tables:\n",
    "        col_meta_df = spark.sql(f\"\"\"\n",
    "            SELECT CONCAT(column_name, ': ', data_type) AS col_desc\n",
    "            FROM {catalog}.information_schema.columns\n",
    "            WHERE table_catalog = '{catalog}'\n",
    "              AND table_schema = '{schema}'\n",
    "              AND table_name = '{table}'\n",
    "        \"\"\")\n",
    "        columns_description = \"\\n\".join([row['col_desc'] for row in col_meta_df.collect()])\n",
    "\n",
    "        prompt = (\n",
    "            f\"You are a data documentation assistant. Given the following columns, infer the purpose and typical use cases of the table. \"\n",
    "            f\"Write a concise, natural-language description of the table's contents and intent, not just a list of fields. \"\n",
    "            f\"Start your description with 'This table...'. \"\n",
    "            f\"For example: 'This table contains records of client activities related to debt collection. It includes details such as the client ID, debtor number, activity date, and notes made by collectors. This data can be used to track collection efforts, analyze collector performance, and understand client interactions over time.'\\n\"\n",
    "            f\"Do not use any special characters \"\n",
    "            f\"Columns:\\n{columns_description}\"\n",
    "        )\n",
    "        escaped_prompt = prompt.replace(\"'\", \"''\")\n",
    "\n",
    "        ai_result = spark.sql(f\"\"\"\n",
    "            SELECT ai_query(\n",
    "              'databricks-llama-4-maverick',\n",
    "              '{escaped_prompt}'\n",
    "            ) AS comment\n",
    "        \"\"\").collect()[0]['comment']\n",
    "\n",
    "        escaped_ai_result = sanitize_comment(ai_result)\n",
    "\n",
    "        table_info = spark.sql(f\"DESCRIBE TABLE EXTENDED `{catalog}`.`{schema}`.`{table}`\")\n",
    "        obj_type = None\n",
    "        is_streaming = False\n",
    "        for row in table_info.collect():\n",
    "            if row['col_name'] == 'Type':\n",
    "                obj_type = str(row['data_type']).upper()\n",
    "                if 'STREAMING' in obj_type:\n",
    "                    is_streaming = True\n",
    "                break\n",
    "\n",
    "        # Try to update comment, and if a parse error occurs, try quoting the table name differently\n",
    "        updated = False\n",
    "        if is_streaming:\n",
    "            print(f\"[SKIPPED] {schema}.{table} is a streaming table. Cannot update comment via SQL.\\n\"\n",
    "                  f\"Suggested comment:\\n{ai_result}\\n\")\n",
    "        elif obj_type and 'VIEW' in obj_type:\n",
    "            print(f\"[SKIPPED] {schema}.{table} is a VIEW. COMMENT ON VIEW not supported.\\n\"\n",
    "                  f\"Suggested comment:\\n{ai_result}\\n\")\n",
    "        elif obj_type and not ('VIEW' in obj_type or 'STREAMING' in obj_type):\n",
    "            try:\n",
    "                # Try with fully quoted identifiers\n",
    "                spark.sql(f\"COMMENT ON TABLE `{catalog}`.`{schema}`.`{table}` IS '{escaped_ai_result}'\")\n",
    "                print(f\"Updated comment for table {schema}.{table}\")\n",
    "                updated = True\n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                # If parse error, try with unquoted table name if it is a valid identifier\n",
    "                if \"[PARSE_SYNTAX_ERROR]\" in error_msg and is_valid_identifier(table):\n",
    "                    try:\n",
    "                        spark.sql(f\"COMMENT ON TABLE {catalog}.{schema}.{table} IS '{escaped_ai_result}'\")\n",
    "                        print(f\"Updated comment for table {schema}.{table} (unquoted fallback)\")\n",
    "                        updated = True\n",
    "                    except Exception as e2:\n",
    "                        print(f\"[ERROR] Failed to update comment for {schema}.{table} (unquoted fallback): {e2}\")\n",
    "                else:\n",
    "                    print(f\"[ERROR] Failed to update comment for {schema}.{table}: {e}\")\n",
    "            if not updated:\n",
    "                print(f\"[SKIPPED] {schema}.{table} could not be updated due to syntax issues. Suggested comment:\\n{ai_result}\\n\")\n",
    "        else:\n",
    "            print(f\"[SKIPPED] {schema}.{table} has unsupported type {obj_type}. Skipping.\")\n",
    "\n",
    "        processed_tables.append((schema, table))\n",
    "\n",
    "if processed_tables:\n",
    "    escaped_table_schema = []\n",
    "    for schema, table in processed_tables:\n",
    "        esc_schema = schema.replace(\"'\", \"''\")\n",
    "        esc_table = table.replace(\"'\", \"''\")\n",
    "        escaped_table_schema.append(f\"(table_schema = '{esc_schema}' AND table_name = '{esc_table}')\")\n",
    "    where_clause = \" OR \".join(escaped_table_schema)\n",
    "    info_schema_comments = spark.sql(f\"\"\"\n",
    "        SELECT table_schema, table_name, comment\n",
    "        FROM {catalog}.information_schema.tables\n",
    "        WHERE table_catalog = '{catalog}'\n",
    "          AND ({where_clause})\n",
    "    \"\"\")\n",
    "    display(info_schema_comments)\n",
    "else:\n",
    "    print('No tables were processed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "647ab2fe-e5a3-45b9-959b-e4712e57a246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bulk AI Generate all Column-level descriptions for a specified Catalog and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45641770-a439-4bf4-b1f4-19945eabb4c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "catalog = 'sandbox'\n",
    "schemas = ['bronze', 'silver', 'gold']\n",
    "\n",
    "def escape_sql_string(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = s.replace(\"'\", \"''\")\n",
    "    s = re.sub(r\"(\\w)''(\\w)\", r\"\\1\\2\", s)\n",
    "    s = s.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    return s.strip()\n",
    "\n",
    "def sanitize_for_sql_comment(s):\n",
    "    # Remove or replace problematic single quotes and special characters that can break SQL parsing\n",
    "    if s is None:\n",
    "        return None\n",
    "    # Replace double single quotes with single quote for clarity, then remove single quotes inside quoted examples\n",
    "    s = re.sub(r\"''([A-Z]+)''\", r\"\\1\", s)\n",
    "    # Remove any remaining single quotes inside the comment\n",
    "    s = s.replace(\"'\", \"\")\n",
    "    # Optionally, remove or replace other problematic characters if needed\n",
    "    return s\n",
    "\n",
    "def is_valid_identifier(name):\n",
    "    return re.match(r'^[A-Za-z_][A-Za-z0-9_]*$', name) is not None\n",
    "\n",
    "processed_columns = []\n",
    "\n",
    "for schema in schemas:\n",
    "    tables_df = spark.sql(f\"\"\"\n",
    "    SELECT DISTINCT table_name\n",
    "    FROM {catalog}.information_schema.tables\n",
    "    WHERE table_catalog = '{catalog}'\n",
    "      AND table_schema = '{schema}'\n",
    "      AND lower(table_name) NOT LIKE 'information_schema%'\n",
    "      AND lower(table_name) NOT LIKE 'sys%'\n",
    "      AND lower(table_name) NOT LIKE 'system%'\n",
    "    \"\"\")\n",
    "    tables = [row['table_name'] for row in tables_df.collect()]\n",
    "    print(f\"Found tables in {schema}: {tables}\")\n",
    "\n",
    "    for table in tables:\n",
    "        table_info = spark.sql(f\"\"\"\n",
    "            DESCRIBE TABLE EXTENDED `{catalog}`.`{schema}`.`{table}`\n",
    "        \"\"\")\n",
    "        is_streaming = False\n",
    "        for row in table_info.collect():\n",
    "            if row['col_name'] == 'Type' and 'STREAMING' in str(row['data_type']).upper():\n",
    "                is_streaming = True\n",
    "                break\n",
    "\n",
    "        table_comment_df = spark.sql(f\"\"\"\n",
    "            SELECT comment FROM {catalog}.information_schema.tables\n",
    "            WHERE table_catalog = '{catalog}'\n",
    "              AND table_schema = '{schema}'\n",
    "              AND table_name = '{table}'\n",
    "        \"\"\")\n",
    "        table_comment = table_comment_df.collect()[0]['comment'] if table_comment_df.count() > 0 else None\n",
    "\n",
    "        columns_df = spark.sql(f\"\"\"\n",
    "            SELECT column_name, data_type, is_nullable\n",
    "            FROM {catalog}.information_schema.columns\n",
    "            WHERE table_catalog = '{catalog}'\n",
    "              AND table_schema = '{schema}'\n",
    "              AND table_name = '{table}'\n",
    "        \"\"\")\n",
    "        columns = columns_df.collect()\n",
    "\n",
    "        for col in columns:\n",
    "            col_name = col['column_name']\n",
    "            col_type = col['data_type']\n",
    "            col_nullable = col['is_nullable']\n",
    "\n",
    "            prompt = (\n",
    "                f\"You are a data documentation assistant. Given the following table and column metadata, \"\n",
    "                f\"infer the meaning and typical use of the column. Write a concise, natural-language \"\n",
    "                f\"description of the column's contents and intent, not just a restatement of the name or type. \"\n",
    "                f\"Start your description with 'This column...'. \"\n",
    "                f\"Do not use any special characters \"\n",
    "                f\"Table: {table}\\n\"\n",
    "                + (f\"Table description: {table_comment}\\n\" if table_comment else \"\")\n",
    "                + f\"Column: {col_name} ({col_type}, {'nullable' if col_nullable == 'YES' else 'not nullable'})\"\n",
    "            )\n",
    "\n",
    "            escaped_prompt = escape_sql_string(prompt)\n",
    "            ai_result_df = spark.sql(f\"\"\"\n",
    "                SELECT ai_query(\n",
    "                    'databricks-llama-4-maverick',\n",
    "                    '{escaped_prompt}'\n",
    "                ) AS comment\n",
    "            \"\"\")\n",
    "            ai_result = ai_result_df.collect()[0]['comment']\n",
    "            # Sanitize the AI result to avoid SQL parse errors due to problematic quoting\n",
    "            sanitized_ai_result = sanitize_for_sql_comment(ai_result)\n",
    "            escaped_ai_result = escape_sql_string(sanitized_ai_result)\n",
    "\n",
    "            if is_streaming:\n",
    "                print(f\"[SKIPPED] {schema}.{table}.{col_name} is a streaming table column. You cannot update its comment via SQL.\\n\"\n",
    "                      f\"To update the comment, edit the column definition in your Lakeflow Declarative Pipeline.\\n\"\n",
    "                      f\"Suggested comment:\\n{ai_result}\\n\")\n",
    "            elif escaped_ai_result and col_name:\n",
    "                updated = False\n",
    "                try:\n",
    "                    spark.sql(f\"ALTER TABLE `{catalog}`.`{schema}`.`{table}` ALTER COLUMN `{col_name}` COMMENT '{escaped_ai_result}'\")\n",
    "                    print(f\"Updated comment for {schema}.{table}.{col_name}\")\n",
    "                    updated = True\n",
    "                except Exception as e:\n",
    "                    error_msg = str(e)\n",
    "                    # Try unquoted fallback if parse error and identifiers are valid\n",
    "                    if \"[PARSE_SYNTAX_ERROR]\" in error_msg and is_valid_identifier(table) and is_valid_identifier(col_name):\n",
    "                        try:\n",
    "                            spark.sql(f\"ALTER TABLE {catalog}.{schema}.{table} ALTER COLUMN {col_name} COMMENT '{escaped_ai_result}'\")\n",
    "                            print(f\"Updated comment for {schema}.{table}.{col_name} (unquoted fallback)\")\n",
    "                            updated = True\n",
    "                        except Exception as e2:\n",
    "                            print(f\"[ERROR] Failed to update comment for {schema}.{table}.{col_name} (unquoted fallback): {e2}\")\n",
    "                    else:\n",
    "                        print(f\"[ERROR] Failed to update comment for {schema}.{table}.{col_name}: {e}\")\n",
    "                if updated:\n",
    "                    processed_columns.append((schema, table, col_name))\n",
    "            else:\n",
    "                print(f\"[SKIPPED] {schema}.{table}.{col_name} - No comment generated or invalid column name.\")\n",
    "\n",
    "if processed_columns:\n",
    "    where_clause = \" OR \".join(\n",
    "        [f\"(table_schema = '{escape_sql_string(s)}' AND table_name = '{escape_sql_string(t)}' AND column_name = '{escape_sql_string(c)}')\"\n",
    "         for s, t, c in processed_columns]\n",
    "    )\n",
    "    info_schema_col_comments = spark.sql(f\"\"\"\n",
    "        SELECT table_schema, table_name, column_name, comment\n",
    "        FROM {catalog}.information_schema.columns\n",
    "        WHERE table_catalog = '{catalog}'\n",
    "          AND ({where_clause})\n",
    "    \"\"\")\n",
    "    display(info_schema_col_comments)\n",
    "else:\n",
    "    print('No columns were processed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb5be112-21ac-44b3-93db-f5f4e601665a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Remove all bulk generated table descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adde0ed2-ed83-48ac-b54d-e67720f8cb95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = 'sandbox'\n",
    "schemas = ['bronze', 'silver', 'gold']\n",
    "\n",
    "processed_tables = []\n",
    "\n",
    "for schema in schemas:\n",
    "    # Get all tables\n",
    "    tables_df = spark.sql(f\"\"\"\n",
    "    SELECT DISTINCT table_name\n",
    "    FROM {catalog}.information_schema.columns\n",
    "    WHERE table_catalog = '{catalog}'\n",
    "    AND table_schema = '{schema}'\n",
    "    \"\"\")\n",
    "    tables = [row['table_name'] for row in tables_df.collect()]\n",
    "\n",
    "    for table in tables:\n",
    "        # Detect if the table is a streaming table\n",
    "        table_info = spark.sql(f\"\"\"\n",
    "            DESCRIBE TABLE EXTENDED {catalog}.{schema}.{table}\n",
    "        \"\"\")\n",
    "        is_streaming = False\n",
    "        for row in table_info.collect():\n",
    "            if row['col_name'] == 'Type' and 'STREAMING' in str(row['data_type']).upper():\n",
    "                is_streaming = True\n",
    "                break\n",
    "\n",
    "        if is_streaming:\n",
    "            print(f\"[SKIPPED] {schema}.{table} is a streaming table. You cannot update its comment via SQL.\\n\"\n",
    "                  f\"To remove the comment, edit the table definition in your Lakeflow Declarative Pipeline.\\n\")\n",
    "        else:\n",
    "            # Remove comment for regular tables\n",
    "            spark.sql(f\"COMMENT ON TABLE {catalog}.{schema}.{table} IS NULL\")\n",
    "            print(f\"Removed comment for table {schema}.{table}\")\n",
    "        processed_tables.append((schema, table))\n",
    "\n",
    "# At the end, show the table and comment from the information schema for processed tables\n",
    "if processed_tables:\n",
    "    # Escape single quotes in table and schema names for SQL\n",
    "    escaped_table_schema = []\n",
    "    for schema, table in processed_tables:\n",
    "        esc_schema = schema.replace(\"'\", \"''\")\n",
    "        esc_table = table.replace(\"'\", \"''\")\n",
    "        escaped_table_schema.append(f\"(table_schema = '{esc_schema}' AND table_name = '{esc_table}')\")\n",
    "    where_clause = \" OR \".join(escaped_table_schema)\n",
    "    info_schema_comments = spark.sql(f\"\"\"\n",
    "        SELECT table_schema, table_name, comment\n",
    "        FROM {catalog}.information_schema.tables\n",
    "        WHERE table_catalog = '{catalog}'\n",
    "        AND ({where_clause})\n",
    "    \"\"\")\n",
    "    display(info_schema_comments)\n",
    "else:\n",
    "    print('No tables were processed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3bc8345-2da5-4933-bc07-3d66567fec40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bulk Remove all Column-level descriptions for a specified Catalog and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "191f2350-4cb3-407a-a1bd-058e1b7ed91f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "catalog = 'sandbox'\n",
    "schemas = ['bronze', 'silver', 'gold']\n",
    "\n",
    "def escape_sql_string(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = s.replace(\"'\", \"''\")\n",
    "    s = re.sub(r\"(\\w)''(\\w)\", r\"\\1\\2\", s)\n",
    "    s = s.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    return s.strip()\n",
    "\n",
    "processed_columns = []\n",
    "\n",
    "for schema in schemas:\n",
    "    # Get all user tables in the schema excluding system tables\n",
    "    tables_df = spark.sql(f\"\"\"\n",
    "    SELECT DISTINCT table_name\n",
    "    FROM {catalog}.information_schema.tables\n",
    "    WHERE table_catalog = '{catalog}'\n",
    "      AND table_schema = '{schema}'\n",
    "      AND lower(table_name) NOT LIKE 'information_schema%'\n",
    "      AND lower(table_name) NOT LIKE 'sys%'\n",
    "      AND lower(table_name) NOT LIKE 'system%'\n",
    "    \"\"\")\n",
    "    tables = [row['table_name'] for row in tables_df.collect()]\n",
    "    print(f\"Found tables in {schema}: {tables}\")\n",
    "\n",
    "    for table in tables:\n",
    "        # Check if the table is a streaming table\n",
    "        table_info = spark.sql(f\"\"\"\n",
    "            DESCRIBE TABLE EXTENDED `{catalog}`.`{schema}`.`{table}`\n",
    "        \"\"\")\n",
    "        is_streaming = False\n",
    "        for row in table_info.collect():\n",
    "            if row['col_name'] == 'Type' and 'STREAMING' in str(row['data_type']).upper():\n",
    "                is_streaming = True\n",
    "                break\n",
    "\n",
    "        # Fetch all columns\n",
    "        columns_df = spark.sql(f\"\"\"\n",
    "            SELECT column_name\n",
    "            FROM {catalog}.information_schema.columns\n",
    "            WHERE table_catalog = '{catalog}'\n",
    "              AND table_schema = '{schema}'\n",
    "              AND table_name = '{table}'\n",
    "        \"\"\")\n",
    "        columns = columns_df.collect()\n",
    "\n",
    "        for col in columns:\n",
    "            col_name = col['column_name']\n",
    "\n",
    "            if is_streaming:\n",
    "                print(f\"[SKIPPED] {schema}.{table}.{col_name} is a streaming table column. You cannot update its comment via SQL.\\n\"\n",
    "                      f\"To remove the comment, edit the column definition in your Lakeflow Declarative Pipeline.\\n\")\n",
    "            elif col_name:\n",
    "                spark.sql(f\"ALTER TABLE `{catalog}`.`{schema}`.`{table}` ALTER COLUMN `{col_name}` COMMENT ''\")\n",
    "                print(f\"Removed comment for {schema}.{table}.{col_name}\")\n",
    "                processed_columns.append((schema, table, col_name))\n",
    "            else:\n",
    "                print(f\"[SKIPPED] {schema}.{table}.{col_name} - Invalid column name.\")\n",
    "\n",
    "# Display updated column comments\n",
    "if processed_columns:\n",
    "    where_clause = \" OR \".join(\n",
    "        [f\"(table_schema = '{escape_sql_string(s)}' AND table_name = '{escape_sql_string(t)}' AND column_name = '{escape_sql_string(c)}')\"\n",
    "         for s, t, c in processed_columns]\n",
    "    )\n",
    "    info_schema_col_comments = spark.sql(f\"\"\"\n",
    "        SELECT table_schema, table_name, column_name, comment\n",
    "        FROM {catalog}.information_schema.columns\n",
    "        WHERE table_catalog = '{catalog}'\n",
    "          AND ({where_clause})\n",
    "    \"\"\")\n",
    "    display(info_schema_col_comments)\n",
    "else:\n",
    "    print('No columns were processed.')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bulk Generate AI Table and Column Comments",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
