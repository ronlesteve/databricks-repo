{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6457c6-8971-4680-a382-2c72d055fa5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ae8d108-2878-4232-801f-96caae8f256e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "from pyspark.sql import Row\n",
    "import random\n",
    "\n",
    "logistics_products = [\n",
    "    \"Freight Forwarding\",\n",
    "    \"Warehousing\",\n",
    "    \"Order Fulfillment\",\n",
    "    \"Last Mile Delivery\",\n",
    "    \"Inventory Management\",\n",
    "    \"Customs Brokerage\",\n",
    "    \"Reverse Logistics\",\n",
    "    \"Cross-Docking\",\n",
    "    \"Distribution Center\",\n",
    "    \"Cold Chain Logistics\",\n",
    "    \"Transportation Management\",\n",
    "    \"Parcel Shipping\",\n",
    "    \"Palletizing\",\n",
    "    \"Load Planning\",\n",
    "    \"EDI Integration\",\n",
    "    \"Returns Processing\",\n",
    "    \"Drayage\",\n",
    "    \"Transloading\",\n",
    "    \"Supply Chain Consulting\",\n",
    "    \"Value-Added Services\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c459fd9-48f8-4cf8-890c-d612aad7e222",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755380298112}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "Faker.seed(42)\n",
    "\n",
    "sample_sales = []\n",
    "for year in [2024, 2025]:\n",
    "    for product in logistics_products:\n",
    "        for month in range(1, 13):\n",
    "            sale_date = f\"{year}-{month:02d}-{random.randint(1,28):02d}\"\n",
    "            sample_sales.append(\n",
    "                Row(\n",
    "                    sale_id=len(sample_sales)+1,\n",
    "                    product_name=product,\n",
    "                    sale_amount=round(\n",
    "                        fake.pyfloat(\n",
    "                            left_digits=4,\n",
    "                            right_digits=2,\n",
    "                            positive=True,\n",
    "                            min_value=20,\n",
    "                            max_value=2000\n",
    "                        ),\n",
    "                        2\n",
    "                    ),\n",
    "                    sales_date=sale_date,\n",
    "                    customer_name=fake.name(),\n",
    "                    customer_address=fake.address().replace('\\n', ', ')\n",
    "                )\n",
    "            )\n",
    "\n",
    "df_sales_sample = spark.createDataFrame(sample_sales)\n",
    "display(df_sales_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de88bcb-3157-47bf-b2cc-f2920d5c8fcb",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755385437181}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, sha2, concat_ws, monotonically_increasing_id\n",
    "\n",
    "# Synthetic: Replace with fake data\n",
    "masked_df_sales = df_sales_sample.withColumn(\n",
    "    \"customer_name\", lit(\"MASKED\")\n",
    ").withColumn(\n",
    "    \"customer_address\", lit(\"MASKED\")\n",
    ")\n",
    "display(masked_df_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "548f5857-f86c-408c-9b78-6e8dbc4b1ce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obfuscation: Replace with hashed/randomized values\n",
    "masked_df_sales_obfuscation = df_sales_sample.withColumn(\n",
    "    \"customer_name\", sha2(concat_ws(\"_\", df_sales_sample.customer_name, monotonically_increasing_id()), 256)\n",
    ").withColumn(\n",
    "    \"customer_address\", sha2(concat_ws(\"_\", df_sales_sample.customer_address, monotonically_increasing_id()), 256)\n",
    ")\n",
    "display(masked_df_sales_obfuscation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d616db0b-1f81-482a-8cfc-364e1ccffcb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Redaction: Blank out the data\n",
    "masked_df_sales_redaction = df_sales_sample.withColumn(\n",
    "    \"customer_name\", lit(\"\")\n",
    ").withColumn(\n",
    "    \"customer_address\", lit(\"\")\n",
    ")\n",
    "display(masked_df_sales_redaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6352199-2633-43b2-b83a-37ad05564597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "_The AI_FORECAST function predicts future values of a time series using historical data.\n",
    "horizon specifies the end date for the forecast.\n",
    "time_col is the column with date information.\n",
    "value_col is the column with numeric values to forecast.\n",
    "prediction_interval_width sets the confidence interval for predictions.\n",
    "parameters allows you to customize the forecasting model (e.g., seasonality mode).\n",
    "You can adjust the horizon and other parameters as needed for your use case._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3312870f-c94e-454e-82ed-9e24eb76d350",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755387210112}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install Prophet if not already installed\n",
    "%pip install prophet\n",
    "\n",
    "from prophet import Prophet\n",
    "\n",
    "# Select columns from Spark DataFrame and convert to Pandas, including both 2024 and 2025 data\n",
    "sales_pd = (\n",
    "  df_sales_sample\n",
    "  .select(\n",
    "    'sales_date',\n",
    "    'sale_amount',\n",
    "    'product_name'\n",
    "  )\n",
    "  .toPandas()\n",
    ")\n",
    "\n",
    "# Prepare data for Prophet\n",
    "sales_pd.rename(\n",
    "  columns = {\n",
    "    'sales_date': 'ds',\n",
    "    'sale_amount': 'y'\n",
    "  },\n",
    "  inplace = True\n",
    ")\n",
    "\n",
    "# Fit Prophet model to sales data for time series forecasting (per product), now including 2024 data\n",
    "forecasts = []\n",
    "for product in sales_pd['product_name'].unique():\n",
    "    product_sales = sales_pd[sales_pd['product_name'] == product][['ds', 'y']]\n",
    "    model = Prophet(seasonality_mode='multiplicative')\n",
    "    model.fit(product_sales)\n",
    "    future = model.make_future_dataframe(periods=365, freq='D')\n",
    "    forecast = model.predict(future)\n",
    "    forecast['product_name'] = product\n",
    "    forecasts.append(forecast[forecast['ds'].dt.year == 2026])\n",
    "\n",
    "import pandas as pd\n",
    "forecast_2026 = pd.concat(forecasts, ignore_index=True)\n",
    "\n",
    "# Move product_name to the beginning\n",
    "cols = ['product_name'] + [col for col in forecast_2025.columns if col != 'product_name']\n",
    "forecast_2026 = forecast_2026[cols]\n",
    "\n",
    "# Display forecast results for 2025 with product name at the beginning\n",
    "display(forecast_2026)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10e1abc0-16d7-4ac1-9cf1-6f6562d49650",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755386224301}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare 2024 actual sales data\n",
    "df_sales_2024_pd = (\n",
    "    df_sales_sample\n",
    "    .select('sales_date', 'product_name', 'sale_amount')\n",
    "    .toPandas()\n",
    ")\n",
    "df_sales_2024_pd['sales_date'] = pd.to_datetime(df_sales_2024_pd['sales_date'])\n",
    "df_sales_2024_pd['year'] = df_sales_2024_pd['sales_date'].dt.year\n",
    "df_sales_2024_pd['month'] = df_sales_2024_pd['sales_date'].dt.month\n",
    "actual_2024 = (\n",
    "    df_sales_2024_pd[df_sales_2024_pd['year'] == 2024]\n",
    "    .groupby(['product_name', 'month'])['sale_amount']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={'sale_amount': 'actual_sales_2024'})\n",
    ")\n",
    "\n",
    "# Prepare 2025 forecast data\n",
    "forecast_2025['month'] = forecast_2025['ds'].dt.month\n",
    "forecast_2025['year'] = forecast_2025['ds'].dt.year\n",
    "forecast_monthly = (\n",
    "    forecast_2025\n",
    "    .groupby(['product_name', 'month'])['yhat']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={'yhat': 'forecast_sales_2025'})\n",
    ")\n",
    "\n",
    "# Merge actual and forecast data\n",
    "comp_df = pd.merge(\n",
    "    actual_2024,\n",
    "    forecast_monthly,\n",
    "    on=['product_name', 'month'],\n",
    "    how='outer'\n",
    ").fillna(0)\n",
    "\n",
    "# Pivot for product/month comparison, move product_name to the beginning\n",
    "pivot_df = comp_df.pivot_table(\n",
    "    index='product_name',\n",
    "    columns='month',\n",
    "    values=['actual_sales_2024', 'forecast_sales_2025'],\n",
    "    aggfunc='sum'\n",
    ")\n",
    "\n",
    "pivot_df = pivot_df.sort_index(axis=1, level=1)\n",
    "pivot_df.reset_index(inplace=True)\n",
    "\n",
    "display(pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2231cfce-abba-4358-8a42-462eba832778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df_sales_forecast_insight = df_sales_forecast.withColumn(\n",
    "    \"forecast_ai_insight\",\n",
    "    expr(\n",
    "        '''\n",
    "        ai_gen(\n",
    "            concat(\n",
    "                \"Generate insights for the following product and sales data. Keep it short, detailed, and concise\",\n",
    "                \"Product: \", product_name, \n",
    "                \". Actual sales amount: \", sale_amount, \n",
    "                \". Forecasted sales: \", forecast_sale_amount, \n",
    "                \". Forecast lower bound: \", forecast_lower, \n",
    "                \". Forecast upper bound: \", forecast_upper, \n",
    "                \". Provide a brief analysis of the sales and forecast.\"\n",
    "            )\n",
    "        )\n",
    "        '''\n",
    "    )\n",
    ")\n",
    "display(df_sales_forecast_insight)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "AI_Sales_Forecast_Insight",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
