{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26e50560-d531-41af-adb8-c24f7591a391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "311d762a-6ed9-42b0-a1fe-01748e24d31a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, expr, current_timestamp\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = (\n",
    "    spark.range(0, 1_000_000)\n",
    "    .withColumn(\"user_id\", (rand() * 10_000).cast(\"int\"))\n",
    "    # Correct event_time calculation using timestamp_seconds to avoid interval error\n",
    "    .withColumn(\"event_time\", expr(\"timestamp_seconds(id % 100000 + unix_timestamp(current_timestamp()))\"))\n",
    "    .withColumn(\"event_type\", (rand() * 100).cast(\"int\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2321b067-35ff-4a92-a257-d46abb4cefef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Catalog and Schema Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a94ef58-277a-4b23-b4de-c41f132584e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS lakeflow_demo\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS lakeflow_demo.bronze\")\n",
    "spark.sql(\"USE CATALOG lakeflow_demo\")\n",
    "spark.sql(\"USE SCHEMA bronze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddc1431a-877c-40ac-8f54-4c968a42b0e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Save Partitioned Table (Managed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d87e1f2-f938-4ce7-844c-7fc0d51e9c1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .partitionBy(\"event_type\") \\\n",
    "  .saveAsTable(\"events_partitioned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d072c73-8ecd-4ce7-b79f-60fd949a83d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Save ZORDER Table (Managed) and Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bbf7808-dd3d-444f-bf4e-0d19b457e2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .saveAsTable(\"events_zorder\")\n",
    "\n",
    "spark.sql(\"OPTIMIZE lakeflow_demo.bronze.events_zorder ZORDER BY (user_id, event_time)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b10fbe0-35a5-4d21-b488-b7f4f54acb95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Save Liquid Clustering Table (Managed) and Enable Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1406b94-1a86-4ff2-bb18-c88e84480fc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .saveAsTable(\"events_liquid_clustered\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE lakeflow_demo.bronze.events_liquid_clustered CLUSTER BY (user_id, event_time)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"OPTIMIZE lakeflow_demo.bronze.events_liquid_clustered FULL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2cba0f8-9fee-4068-b120-1337cee6f2e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Benchmark Full Table Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c84cab-2671-422c-8cc7-e4c5ae6e721d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark(table_name):\n",
    "    start = time.time()\n",
    "    spark.sql(f\"SELECT COUNT(*) FROM {table_name}\").collect()\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"{table_name} scanned in {elapsed:.2f} seconds\")\n",
    "\n",
    "tables = [\n",
    "    \"lakeflow_demo.bronze.events_partitioned\",\n",
    "    \"lakeflow_demo.bronze.events_zorder\",\n",
    "    \"lakeflow_demo.bronze.events_liquid_clustered\"\n",
    "]\n",
    "\n",
    "for t in tables:\n",
    "    benchmark(t)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Liquid Clustering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
